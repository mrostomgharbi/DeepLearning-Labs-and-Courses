{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face verification using Siamese Networks\n",
    "\n",
    "### Goals\n",
    "- train a network for face similarity using siamese networks\n",
    "- work data augmentation, generators and hard negative mining\n",
    "- use the model on your picture\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "- We will be using Labeled Faces in the Wild (LFW) dataset available openly at http://vis-www.cs.umass.edu/lfw/\n",
    "- For computing purposes, we'll only restrict ourselves to a subpart of the dataset. You're welcome to train on the whole dataset on GPU, by setting `USE_SUBSET=True` in the following cells,\n",
    "- We will also load pretrained weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import os\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Concatenate, merge, Lambda, Dot\n",
    "from keras.layers import Conv2D, MaxPool2D, GlobalAveragePooling2D, Flatten, Dropout\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the dataset\n",
    "\n",
    "The dataset consists of folders corresponding to each identity. The folder name is the name of the person.\n",
    "We map each class (identity) to an integer id, and build mappings as dictionaries `name_to_classid` and `classid_to_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"lfw/lfw-deepfunneled/\"\n",
    "USE_SUBSET = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = sorted(os.listdir(PATH))\n",
    "if USE_SUBSET:\n",
    "    dirs = dirs[:500]\n",
    "name_to_classid = {d: i for i, d in enumerate(dirs)}\n",
    "classid_to_name = {v: k for k, v in name_to_classid.items()}\n",
    "num_classes = len(name_to_classid)\n",
    "print(\"number of classes: \", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each directory, there is one or more images corresponding to the identity. We map each image path with an integer id, then build a few dictionaries:\n",
    "- mappings from imagepath and image id: `path_to_id` and `id_to_path`\n",
    "- mappings from class id to image ids: `classid_to_ids` and `id_to_classid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all directories\n",
    "img_paths = {c: [PATH + subfolder + \"/\" + img\n",
    "                 for img in sorted(os.listdir(PATH + subfolder))] \n",
    "             for subfolder, c in name_to_classid.items()}\n",
    "\n",
    "# retrieve all images\n",
    "all_images_path = []\n",
    "for img_list in img_paths.values():\n",
    "    all_images_path += img_list\n",
    "\n",
    "# map to integers\n",
    "path_to_id = {v: k for k, v in enumerate(all_images_path)}\n",
    "id_to_path = {v: k for k, v in path_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build mappings between images and class\n",
    "classid_to_ids = {k: [path_to_id[path] for path in v] for k, v in img_paths.items()}\n",
    "id_to_classid = {v: c for c,imgs in classid_to_ids.items() for v in imgs}\n",
    "dict(list(id_to_classid.items())[0:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following histogram shows the number of images per class: there are many classes with only one image. \n",
    "These classes are useful as negatives, only as we can't make a positive pair with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(classid_to_name[x], len(classid_to_ids[x]))\n",
    " for x in np.argsort([len(v) for k,v in classid_to_ids.items()])[::-1][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist([len(v) for k,v in classid_to_ids.items()], bins=range(1,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(classid_to_name[x], len(classid_to_ids[x]))\n",
    " for x in np.argsort([len(v) for k,v in classid_to_ids.items()])[::-1][:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese nets\n",
    "\n",
    "A siamese net takes as input two images $x_1$ and $x_2$ and outputs a single value which corresponds to the similarity between $x_1$ and $x_2$, as follows:\n",
    "\n",
    "<img src=\"images/siamese.svg\" style=\"width: 600px;\" />\n",
    "\n",
    "\n",
    "\n",
    "In order to train such a system, one has to build positive and negative pairs for the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pairs of positive image ids for a given classid\n",
    "def build_pos_pairs_for_id(classid, max_num=50):\n",
    "    imgs = classid_to_ids[classid]\n",
    "    if len(imgs) == 1:\n",
    "        return []\n",
    "    pos_pairs = [(imgs[i], imgs[j])\n",
    "                 for i in range(len(imgs))\n",
    "                 for j in range(i + 1, len(imgs))]\n",
    "    random.shuffle(pos_pairs)\n",
    "    return pos_pairs[:max_num]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pairs of negative image ids for a given classid\n",
    "def build_neg_pairs_for_id(classid, classes, max_num=20):\n",
    "    imgs = classid_to_ids[classid]\n",
    "    neg_classes_ids = random.sample(classes, max_num+1)\n",
    "    if classid in neg_classes_ids:\n",
    "        neg_classes_ids.remove(classid)\n",
    "    neg_pairs = []\n",
    "    for id2 in range(max_num):\n",
    "        img1 = imgs[random.randint(0,len(imgs)-1)]\n",
    "        imgs2 = classid_to_ids[neg_classes_ids[id2]]\n",
    "        img2 = imgs2[random.randint(0,len(imgs2)-1)]\n",
    "        neg_pairs += [(img1, img2)]\n",
    "    return neg_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build positive and a negative pairs for class 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_pos_pairs_for_id(5, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_neg_pairs_for_id(5, list(range(num_classes)), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way to compute the pairs, let's open all the possible images. It will expand all the images into RAM memory. There are more than 1000 images, so 100Mo of RAM will be used, which will not cause any issue.\n",
    "\n",
    "_Note: if you plan on opening more images, you should not open them all at once, and rather build a generator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "def resize100(img):\n",
    "    return resize(img, (100, 100), preserve_range=True, mode='reflect', anti_aliasing=True)[20:80,20:80,:]\n",
    "\n",
    "\n",
    "def open_all_images(id_to_path):\n",
    "    all_imgs = []\n",
    "    for path in id_to_path.values():\n",
    "        all_imgs += [np.expand_dims(resize100(imread(path)),0)]\n",
    "    return np.vstack(all_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs = open_all_images(id_to_path)\n",
    "all_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(all_imgs.nbytes / 1e6) + \"MB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function builds a large number of positives/negatives pairs (train and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_test_data(split=0.8):\n",
    "    listX1 = []\n",
    "    listX2 = []\n",
    "    listY = []\n",
    "    split = int(num_classes * split)\n",
    "    \n",
    "    # train\n",
    "    for id in range(split):\n",
    "        pos = build_pos_pairs_for_id(id)\n",
    "        neg = build_neg_pairs_for_id(id, list(range(split)))\n",
    "        for pair in pos:\n",
    "            listX1 += [pair[0]]\n",
    "            listX2 += [pair[1]]\n",
    "            listY += [1]\n",
    "        for pair in neg:\n",
    "            if sum(listY) > len(listY) / 2:\n",
    "                listX1 += [pair[0]]\n",
    "                listX2 += [pair[1]]\n",
    "                listY += [0]\n",
    "    perm = np.random.permutation(len(listX1))\n",
    "    X1_ids_train = np.array(listX1)[perm]\n",
    "    X2_ids_train = np.array(listX2)[perm]\n",
    "    Y_ids_train = np.array(listY)[perm]\n",
    "    \n",
    "    listX1 = []\n",
    "    listX2 = []\n",
    "    listY = []\n",
    "    \n",
    "    #test\n",
    "    for id in range(split, num_classes):\n",
    "        pos = build_pos_pairs_for_id(id)\n",
    "        neg = build_neg_pairs_for_id(id, list(range(split, num_classes)))\n",
    "        for pair in pos:\n",
    "            listX1 += [pair[0]]\n",
    "            listX2 += [pair[1]]\n",
    "            listY += [1]\n",
    "        for pair in neg:\n",
    "            if sum(listY) > len(listY) / 2:\n",
    "                listX1 += [pair[0]]\n",
    "                listX2 += [pair[1]]\n",
    "                listY += [0]\n",
    "    X1_ids_test = np.array(listX1)\n",
    "    X2_ids_test = np.array(listX2)\n",
    "    Y_ids_test = np.array(listY)\n",
    "    return (X1_ids_train, X2_ids_train, Y_ids_train,\n",
    "            X1_ids_test, X2_ids_test, Y_ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_ids_train, X2_ids_train, train_Y, X1_ids_test, X2_ids_test, test_Y = build_train_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_ids_train.shape, X2_ids_train.shape, train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_ids_test.shape, X2_ids_test.shape, test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data augmentation and generator**\n",
    "\n",
    "We're building a generator, which will modify images through dataaugmentation on the fly. \n",
    "The generator enables\n",
    "We use iaa library which offers tremendous possibilities for data augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5), # horizontally flip 50% of the images\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "\n",
    "    def __init__(self, X1, X2, Y, batch_size, all_imgs):\n",
    "        self.cur_train_index=0\n",
    "        self.batch_size = batch_size\n",
    "        self.X1 = X1\n",
    "        self.X2 = X2\n",
    "        self.Y = Y\n",
    "        self.imgs = all_imgs\n",
    "        self.num_samples = Y.shape[0]\n",
    "        \n",
    "    def next_train(self):\n",
    "        while 1:\n",
    "            self.cur_train_index += self.batch_size\n",
    "            if self.cur_train_index >= self.num_samples:\n",
    "                self.cur_train_index=0\n",
    "            \n",
    "            imgs1 = self.X1[self.cur_train_index:self.cur_train_index+self.batch_size]\n",
    "            imgs2 = self.X2[self.cur_train_index:self.cur_train_index+self.batch_size]\n",
    "    \n",
    "       # deactivate augmentation\n",
    "       #     yield ([self.imgs[imgs1], self.imgs[imgs2]],\n",
    "       #             self.Y[self.cur_train_index:self.cur_train_index+self.batch_size])\n",
    "        \n",
    "            yield ([seq.augment_images(self.imgs[imgs1]), \n",
    "                    seq.augment_images(self.imgs[imgs2])],\n",
    "                   self.Y[self.cur_train_index:self.cur_train_index+self.batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(X1_ids_train, X2_ids_train, train_Y, 32, all_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x1, x2], y = next(gen.next_train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 6, i + 1)\n",
    "    plt.imshow(x1[i] / 255)\n",
    "    plt.axis('off')\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 6, i + 7)\n",
    "    plt.imshow(x2[i] / 255)\n",
    "    if y[i]==1.0:\n",
    "        plt.title(\"similar\")\n",
    "    else:\n",
    "        plt.title(\"different\")\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "- Add your own dataaugmentations in the process. You may look at: http://imgaug.readthedocs.io for instance use `iaa.Affine`;\n",
    "- Be careful not to make the task to difficult, and to add meaningful augmentations;\n",
    "- Rerun the generator plot above to check whether the image pairs look not too distorted to recognize the identities.\n",
    "\n",
    "** Test images **\n",
    "\n",
    "- In addition to our generator, we need test images, unaffected by the augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X1 = all_imgs[X1_ids_test]\n",
    "test_X2 = all_imgs[X2_ids_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_X1.shape, test_X2.shape, test_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple convolutional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred, margin=0.25):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    return K.mean( y_true * K.square(1 - y_pred) +\n",
    "                  (1 - y_true) * K.square(K.maximum(y_pred - margin, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_sim(y_true, y_pred, threshold=0.5):\n",
    "    '''Compute classification accuracy with a fixed threshold on similarity.\n",
    "    '''\n",
    "    y_thresholded = K.cast(y_pred > threshold, y_true.dtype)\n",
    "    return K.mean(K.equal(y_true, y_thresholded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "- Build a convolutional model which transforms the input to a fixed dimension $d = 50$\n",
    "- You may alternate convolutions and maxpooling and layers,\n",
    "- Use the relu activation on convolutional layers,\n",
    "- At the end, `Flatten` the last convolutional output and plug it into a dense layer.\n",
    "- Feel free to use some `Dropout` prior to the Dense layer.\n",
    "\n",
    "Use between 32 and 128 channels on convolutional layers. Be careful: large convolutions on high dimensional images can be very slow on CPUs.\n",
    "\n",
    "Try to run your randomly initialized `shared_conv`  model on a batch of the first 10 images from `all_imgs`. What is the expected shape of the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input((60, 60, 3), dtype='float32')\n",
    "\n",
    "# TODO:\n",
    "x = inp\n",
    "\n",
    "shared_conv = Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/shared_conv.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_conv.predict(all_imgs[:10]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shared_conv.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Assemble the siamese model by combining:\n",
    "\n",
    "- `shared_conv` on both inputs;\n",
    "- compute the cosine similarity using the [Dot](https://keras.io/layers/merge/#dot) layer with `normalize=True` on the outputs of the two `shared_conv` instance lanes;\n",
    "- the loss of siamese model is the constrastive loss defined previously;\n",
    "- use the `accuracy_sim` function defined previously as a metric.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = Input((60, 60, 3), dtype='float32')\n",
    "i2 = Input((60, 60, 3), dtype='float32')\n",
    "\n",
    "# TODO: implement me!\n",
    "out = None\n",
    "\n",
    "model = Model(inputs=[i1, i2], outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/siamese.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit the model and checkpoint it to keep the best version. We can expect to get a model with around 0.75 as \"accuracy_sim\" on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "best_model_fname = \"siamese_checkpoint.h5\"\n",
    "best_model_cb = ModelCheckpoint(best_model_fname, monitor='val_accuracy_sim',\n",
    "                                save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(generator=gen.next_train(), \n",
    "                    steps_per_epoch=train_Y.shape[0] // 32, \n",
    "                    epochs=15,\n",
    "                    validation_data=([test_X1, test_X2], test_Y),\n",
    "                    callbacks=[best_model_cb], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"siamese_checkpoint.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may load a pre-trained model if you have the exact solution architecture. \n",
    "# This model is a start, but far from perfect !\n",
    "# model.load_weights(\"siamese_pretrained.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Finding the most similar images\n",
    "\n",
    "- Run the shared_conv model on all images;\n",
    "- (Optional) add Charles and Olivier's faces from the `test_images` folder to the test set;\n",
    "- build a `most_sim` function which returns the most similar vectors to a given vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "emb = None\n",
    "\n",
    "def most_sim(x, emb, topn=3):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/most_similar.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Most similar faces **\n",
    "\n",
    "The following enables to display an image alongside with most similar images:\n",
    "\n",
    "- The results are weak, first because of the size of the dataset\n",
    "- Also, the network can be greatly improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(img):\n",
    "    img = img.astype('uint8')\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_classes = list(filter(lambda x: len(x[1]) > 4, classid_to_ids.items()))\n",
    "class_id = random.choice(interesting_classes)[0]\n",
    "\n",
    "query_id = random.choice(classid_to_ids[class_id])\n",
    "print(\"query:\", classid_to_name[class_id], query_id)\n",
    "# display(all_imgs[query_id])\n",
    "\n",
    "print(\"nearest matches\")\n",
    "for result_id, sim in most_sim(emb[query_id], emb):\n",
    "    class_name = classid_to_name.get(id_to_classid.get(result_id))\n",
    "    print(class_name, result_id, sim)    \n",
    "    display(all_imgs[result_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this model is still underfitting, even when running queries against the training set. Even if the results are not correct, the mistakes often seem to \"make sense\" though.\n",
    "\n",
    "Running a model to convergence on higher resolution images, possibly with a deeper and wider convolutional network might yield better results. In the next notebook we will try with a better loss and with hard negative mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Playing with the camera**\n",
    "- The following code enables you to find the most similar faces to yours\n",
    "- What do you observe?\n",
    "- Try to think of reasons why it doesn't work very well, and how you could improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def camera_grab(camera_id=0, fallback_filename=None):\n",
    "    camera = cv2.VideoCapture(camera_id)\n",
    "    try:\n",
    "        # take 10 consecutive snapshots to let the camera automatically tune\n",
    "        # itself and hope that the contrast and lightning of the last snapshot\n",
    "        # is good enough.\n",
    "        for i in range(10):\n",
    "            snapshot_ok, image = camera.read()\n",
    "        if snapshot_ok:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            print(\"WARNING: could not access camera\")\n",
    "            if fallback_filename:\n",
    "                image = imread(fallback_filename)\n",
    "    finally:\n",
    "        camera.release()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = camera_grab(camera_id=0,\n",
    "                    fallback_filename='test_images/olivier/img_olivier_0.jpeg')\n",
    "x = resize100(image)\n",
    "out = shared_conv.predict(np.reshape(x, (1, 60, 60, 3)))\n",
    "print(\"query image:\")\n",
    "display(x)\n",
    "for id, sim in most_sim(out[0], emb, topn=10):\n",
    "    class_name = classid_to_name.get(id_to_classid.get(id))\n",
    "    print(class_name, id, sim)\n",
    "    display(all_imgs[id])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
