{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "\n",
    "def f(X, noise=0.4, random_state=0):\n",
    "    x = X[:, 0]\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    return np.sin(2 * np.pi * x) + rng.randn(x.shape[0]) * noise\n",
    "\n",
    "\n",
    "def make_data(n, random_state=0):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n_sin = int(0.9 * n)\n",
    "    n_zeros = int(n - n_sin)\n",
    "    X1 = rng.uniform(0, 1, size=(n_sin, 1))\n",
    "    y1 = f(X1, random_state=random_state)\n",
    "    X2 = rng.uniform(-1, 0, size=(n_zeros, 1))\n",
    "    y2 = np.zeros(n_zeros, dtype=y1.dtype)\n",
    "    return shuffle(np.vstack([X1, X2]), np.concatenate([y1, y2]),\n",
    "                   random_state=rng)\n",
    "\n",
    "\n",
    "X_train, y_train = make_data(200, random_state=0)\n",
    "X_test, y_test = make_data(10000, random_state=1)\n",
    "\n",
    "\n",
    "plt.scatter(X_train[:, 0], y_train)\n",
    "plt.savefig('noisy_sin_dataset.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_range = [2, 4, 8, 16, 32, 64]\n",
    "tree = DecisionTreeRegressor()\n",
    "train_scores, test_scores = validation_curve(\n",
    "    DecisionTreeRegressor(),\n",
    "    X_train, y_train, param_name=\"max_leaf_nodes\", param_range=param_range,\n",
    "    cv=5, scoring=\"neg_mean_squared_error\", n_jobs=1)\n",
    "train_scores_mean = -np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = -np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Decision Tree\")\n",
    "plt.xlabel(\"number of nodes\")\n",
    "plt.ylabel(\"Score\")\n",
    "lw = 2\n",
    "plt.plot(param_range, train_scores_mean, label=\"Training score\",\n",
    "         color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "         color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.ylim(0, 0.8)\n",
    "plt.xlim(param_range[0], param_range[-1])\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, max_leaf_nodes in enumerate(param_range):\n",
    "    fig, (left, right) = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "    right.scatter(X_train[:, 0], y_train, alpha=0.3)\n",
    "    left.set_title(\"Decision Tree\")\n",
    "    left.set_xlabel(\"number of nodes\")\n",
    "    left.set_ylabel(\"Score\")\n",
    "    lw = 2\n",
    "    left.plot(param_range[:i + 1], train_scores_mean[:i + 1], label=\"Training score\",\n",
    "              color=\"darkorange\", marker='o', lw=lw)\n",
    "    left.fill_between(param_range[:i + 1], train_scores_mean[:i + 1] - train_scores_std[:i + 1],\n",
    "                      train_scores_mean[:i + 1] + train_scores_std[:i + 1], alpha=0.2,\n",
    "                      color=\"darkorange\", lw=lw)\n",
    "    left.plot(param_range[:i + 1], test_scores_mean[:i + 1], label=\"Cross-validation score\",\n",
    "              color=\"navy\", marker='o', lw=lw)\n",
    "    left.fill_between(param_range[:i + 1], test_scores_mean[:i + 1] - test_scores_std[:i + 1],\n",
    "                      test_scores_mean[:i + 1] + test_scores_std[:i + 1], alpha=0.2,\n",
    "                      color=\"navy\", lw=lw)\n",
    "    left.set_ylim(0, 0.8)\n",
    "    left.set_xlim(param_range[0], param_range[-1])\n",
    "    left.legend(loc=\"best\");\n",
    "    \n",
    "    tree.max_leaf_nodes = max_leaf_nodes\n",
    "    tree.fit(X_train, y_train)\n",
    "    x = np.linspace(-1.2, 1.2)\n",
    "    right.plot(x, tree.predict(x[:, None]))\n",
    "    right.set_xlabel('x')\n",
    "    right.set_ylabel('y = model(x)')\n",
    "    right.set_title('Model predictions')\n",
    "    right.set_ylim(-2, 2)\n",
    "    \n",
    "    fig.savefig(f'noisy_sin_tree_{i:03d}.svg')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "poly = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures()),\n",
    "    ('ridge', Ridge(alpha=1e-6)),\n",
    "])\n",
    "\n",
    "param_range = [0, 5, 10, 15, 20, 25]\n",
    "train_scores, test_scores = validation_curve(\n",
    "    poly, X_train, y_train, param_name=\"poly_features__degree\",\n",
    "    param_range=param_range, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=8)\n",
    "train_scores_mean = -np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = -np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Polynomial Regression\")\n",
    "plt.xlabel(\"degree\")\n",
    "plt.ylabel(\"Score\")\n",
    "lw = 2\n",
    "plt.plot(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.ylim(0, 0.8)\n",
    "plt.xlim(param_range[0], param_range[-1])\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, degree in enumerate(param_range):\n",
    "    fig, (left, right) = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "    right.scatter(X_train[:, 0], y_train, alpha=0.3)\n",
    "    left.set_title(\"Polynomial Regression\")\n",
    "    left.set_xlabel(\"degree\")\n",
    "    left.set_ylabel(\"Score\")\n",
    "    lw = 2\n",
    "    left.plot(param_range[:i + 1], train_scores_mean[:i + 1], label=\"Training score\",\n",
    "              color=\"darkorange\", marker='o', lw=lw)\n",
    "    left.fill_between(param_range[:i + 1], train_scores_mean[:i + 1] - train_scores_std[:i + 1],\n",
    "                      train_scores_mean[:i + 1] + train_scores_std[:i + 1], alpha=0.2,\n",
    "                      color=\"darkorange\", lw=lw)\n",
    "    left.plot(param_range[:i + 1], test_scores_mean[:i + 1], label=\"Cross-validation score\",\n",
    "              color=\"navy\", marker='o', lw=lw)\n",
    "    left.fill_between(param_range[:i + 1], test_scores_mean[:i + 1] - test_scores_std[:i + 1],\n",
    "                      test_scores_mean[:i + 1] + test_scores_std[:i + 1], alpha=0.2,\n",
    "                      color=\"navy\", lw=lw)\n",
    "    left.set_ylim(0, 0.8)\n",
    "    left.set_xlim(param_range[0], param_range[-1])\n",
    "    left.legend(loc=\"best\");\n",
    "    \n",
    "    poly.set_params(poly_features__degree=degree)\n",
    "    poly.fit(X_train, y_train)\n",
    "    x = np.linspace(-1.2, 1.2)\n",
    "    right.plot(x, poly.predict(x[:, None]))\n",
    "    right.set_xlabel('x')\n",
    "    right.set_ylabel('y = model(x)')\n",
    "    right.set_title('Model predictions')\n",
    "    right.set_ylim(-2, 2)\n",
    "    \n",
    "    fig.savefig(f'noisy_sin_poly_{i:03d}.svg')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# mlp = MLPRegressor(solver='lbfgs', max_iter=5000, alpha=0)\n",
    "mlp = MLPRegressor(solver='adam', learning_rate_init=0.1, learning_rate='adaptive',\n",
    "                   alpha=0, max_iter=5000, n_iter_no_change=100, random_state=0)\n",
    "\n",
    "param_range = [16, 32, 64, 128, 256, 1024]\n",
    "train_scores, test_scores = validation_curve(\n",
    "    mlp, X_train, y_train, param_name=\"hidden_layer_sizes\",\n",
    "    param_range=param_range, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=2)\n",
    "\n",
    "train_scores_mean = -np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = -np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"1 hidden layers MLP (no regularization)\")\n",
    "plt.xlabel(\"width\")\n",
    "plt.ylabel(\"Score\")\n",
    "lw = 2\n",
    "plt.plot(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.ylim(0, 0.8)\n",
    "plt.xlim(param_range[0], 256)\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, h in enumerate(param_range[:-1]):\n",
    "    fig, (left, right) = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "    right.scatter(X_train[:, 0], y_train, alpha=0.3)\n",
    "    left.set_title(\"Single hidden layer MLP, no regularization\")\n",
    "    left.set_xlabel(\"hidden units\")\n",
    "    left.set_ylabel(\"Score\")\n",
    "    lw = 2\n",
    "    left.plot(param_range[:i + 1], train_scores_mean[:i + 1], label=\"Training score\",\n",
    "              color=\"darkorange\", marker='o', lw=lw)\n",
    "    left.fill_between(param_range[:i + 1], train_scores_mean[:i + 1] - train_scores_std[:i + 1],\n",
    "                      train_scores_mean[:i + 1] + train_scores_std[:i + 1], alpha=0.2,\n",
    "                      color=\"darkorange\", lw=lw)\n",
    "    left.plot(param_range[:i + 1], test_scores_mean[:i + 1], label=\"Cross-validation score\",\n",
    "              color=\"navy\", marker='o', lw=lw)\n",
    "    left.fill_between(param_range[:i + 1], test_scores_mean[:i + 1] - test_scores_std[:i + 1],\n",
    "                      test_scores_mean[:i + 1] + test_scores_std[:i + 1], alpha=0.2,\n",
    "                      color=\"navy\", lw=lw)\n",
    "    left.set_ylim(0, 0.8)\n",
    "    left.set_xlim(param_range[0], param_range[-2])\n",
    "    left.legend(loc=\"best\");\n",
    "    \n",
    "    mlp.set_params(hidden_layer_sizes=[h])\n",
    "    mlp.fit(X_train, y_train)\n",
    "    x = np.linspace(-1.2, 1.2)\n",
    "    right.plot(x, mlp.predict(x[:, None]))\n",
    "    right.set_xlabel('x')\n",
    "    right.set_ylabel('y = model(x)')\n",
    "    right.set_title('Model predictions')\n",
    "    right.set_ylim(-2, 2)\n",
    "    \n",
    "    fig.savefig(f'noisy_sin_mlp_{i:03d}.svg')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.title(\"1 hidden layers MLP (no regularization)\")\n",
    "plt.xlabel(\"width\")\n",
    "plt.ylabel(\"Score\")\n",
    "lw = 2\n",
    "plt.plot(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.ylim(0, 0.8)\n",
    "plt.xlim(param_range[0], param_range[-1])\n",
    "plt.legend(loc=\"best\");\n",
    "plt.savefig(f'noisy_sin_mlp_full_learning_curve.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
