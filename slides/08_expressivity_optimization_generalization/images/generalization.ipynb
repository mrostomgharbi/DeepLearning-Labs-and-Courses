{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 0.15\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train, y_train = make_moons(n_samples=30, shuffle=True, noise=noise, random_state=rng)\n",
    "X_test, y_test = make_moons(n_samples=1000, shuffle=True, noise=noise, random_state=rng)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "noised_versions = []\n",
    "for _ in range(5):\n",
    "    X_noise, y_noise = make_moons(n_samples=30, shuffle=True, noise=noise, random_state=rng)\n",
    "    X_noise = scaler.transform(X_noise)\n",
    "    y_noise = shuffle(y_noise, random_state=rng)\n",
    "    noised_versions.append(shuffle(np.vstack([X_train, X_noise]),\n",
    "                                   np.concatenate([y_train, y_noise]),\n",
    "                                   random_state=rng))\n",
    "    \n",
    "X_train_noise, y_train_noise = noised_versions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(ax, clf, xlim=(-2.5, 2.5), ylim=(-2.5, 2.5),\n",
    "               steps=100, cmap=plt.cm.RdBu, alpha=.5):\n",
    "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], steps),\n",
    "                         np.linspace(ylim[0], ylim[1], steps))\n",
    "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=cmap, alpha=alpha)\n",
    "    ax.set_xlim(xlim); ax.set_ylim(ylim)\n",
    "    \n",
    "    \n",
    "def plot_model_data(ax, X, y, clf=None,\n",
    "                    xlim=(-2.5, 2.5), ylim=(-2.5, 2.5),\n",
    "                    steps=100, cmap=plt.cm.RdBu, alpha=.5):\n",
    "    if clf is not None:\n",
    "        plot_model(ax, clf, xlim=xlim, ylim=ylim, steps=steps,\n",
    "                   cmap=cmap, alpha=alpha)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, s=60,\n",
    "               edgecolors='k',alpha=.7)\n",
    "    ax.set_xticks([]), ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(m, X_train, y_train, X_test, y_test):\n",
    "    print(f\"train acc: {m.score(X_train, y_train) * 100:.0f}%,\"\n",
    "          f\" test acc: {m.score(X_test, y_test) * 100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_params = dict(\n",
    "    learning_rate_init=0.1,\n",
    "    learning_rate='constant',\n",
    "    max_iter=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_lbfgs = MLPClassifier(solver='lbfgs', hidden_layer_sizes=[256],\n",
    "                   alpha=0, random_state=0)\n",
    "m_lbfgs.fit(X_train, y_train)\n",
    "evaluate(m_lbfgs, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_sgd = MLPClassifier(solver='sgd', hidden_layer_sizes=[256],\n",
    "                   alpha=0, random_state=0, **sgd_params)\n",
    "m_sgd.fit(X_train, y_train)\n",
    "evaluate(m_sgd, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_list = plt.subplots(nrows=2, ncols=4, figsize=(16, 8))\n",
    "stats = []\n",
    "for i, ax in enumerate(ax_list.ravel()):\n",
    "    hidden_size = 2 ** (i + 2)\n",
    "    m = MLPClassifier(solver='sgd', hidden_layer_sizes=[hidden_size],\n",
    "                   alpha=0, random_state=0, **sgd_params)\n",
    "    m.fit(X_train, y_train)\n",
    "    train_acc = m.score(X_train, y_train) * 100\n",
    "    test_acc = m.score(X_test, y_test) * 100\n",
    "    stats.append(dict(\n",
    "        n_layers=1,\n",
    "        hidden_size=hidden_size,\n",
    "        train_acc=train_acc,\n",
    "        test_acc=test_acc,\n",
    "    ))\n",
    "    ax.set_title(f\"hidden: {hidden_size}, train {train_acc:.1f}%, test {test_acc:.1f}%\")\n",
    "    plot_model_data(ax, X_train, y_train, clf=m)\n",
    "fig.suptitle(\"1 hidden layer MLP\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_list = plt.subplots(nrows=2, ncols=4, figsize=(16, 8))\n",
    "for i, ax in enumerate(ax_list.ravel()):\n",
    "    hidden_size = 2 ** (i + 2)\n",
    "    m = MLPClassifier(solver='lbfgs', hidden_layer_sizes=[hidden_size] * 2,\n",
    "                      alpha=0, random_state=0, **sgd_params)\n",
    "    m.fit(X_train, y_train)\n",
    "    train_acc = m.score(X_train, y_train) * 100\n",
    "    test_acc = m.score(X_test, y_test) * 100\n",
    "    stats.append(dict(\n",
    "        n_layers=2,\n",
    "        hidden_size=hidden_size,\n",
    "        train_acc=train_acc,\n",
    "        test_acc=test_acc,\n",
    "    ))\n",
    "    ax.set_title(f\"hidden: {hidden_size}, train {train_acc:.1f}%, test {test_acc:.1f}%\")\n",
    "    plot_model_data(ax, X_train, y_train, clf=m)\n",
    "fig.suptitle(\"2 hidden layers MLP\");\n",
    "fig.savefig('moons_lbfgs.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame.from_dict(stats)\n",
    "stats_df = stats_df.sort_values('hidden_size')\n",
    "# stats_df[stats_df['n_layers'] == 1].plot(x='hidden_size', y='train_acc')\n",
    "# stats_df[stats_df['n_layers'] == 1].plot(x='hidden_size', y='test_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(16, 8))\n",
    "plot_model_data(ax0, X_train, y_train)\n",
    "ax0.set_title(\"Original training set: m=50\")\n",
    "\n",
    "plot_model_data(ax1, X_train_noise, y_train_noise)\n",
    "ax1.set_title(\"Original training set + noise point: m=50 + 100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "m_smooth = MLPClassifier(solver='lbfgs', hidden_layer_sizes=[256],\n",
    "                   alpha=0, random_state=0)\n",
    "m_smooth.fit(X_train, y_train)\n",
    "evaluate(m_smooth, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plot_model_data(ax, X_train, y_train)\n",
    "fig.savefig(\"half_moons.svg\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plot_model_data(ax, X_train, y_train, m_smooth)\n",
    "fig.savefig(\"low_complexity_decision_function.svg\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "m_noise = clone(m_smooth)\n",
    "m_noise.fit(X_train_noise, y_train_noise)\n",
    "evaluate(m_noise, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for X, y  in noised_versions:\n",
    "    m = clone(m_smooth)\n",
    "    m.fit(X, y)\n",
    "    if m.score(X_train, y_train) == 1:\n",
    "        i += 1\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        plot_model_data(ax, X, y, m)\n",
    "        fig.savefig(f\"high_complexity_decision_function_{i:02d}.svg\")\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn.neural_network.multilayer_perceptron import ACTIVATIONS\n",
    "\n",
    "\n",
    "def logits(m, X):\n",
    "    sigma = ACTIVATIONS[m.activation]\n",
    "    a = X\n",
    "    for i in range(m.n_layers_ - 1):\n",
    "        a = safe_sparse_dot(a, m.coefs_[i])\n",
    "        a += m.intercepts_[i]\n",
    "        if (i + 1) != (m.n_layers_ - 1):\n",
    "            activations = sigma(a)\n",
    "    return a\n",
    "\n",
    "\n",
    "def lipschitz(m):\n",
    "    return np.prod([max(np.linalg.svd(w)[1]) for w in m.coefs_])\n",
    "\n",
    "\n",
    "def margins(m, X, y):\n",
    "    preds = logits(m, X).ravel()\n",
    "#     correct_mask = (preds >= 0) == y\n",
    "#     return np.abs(preds * correct_mask)\n",
    "    return np.abs(preds)\n",
    "\n",
    "\n",
    "def normalized_margins(m, X, y):\n",
    "    return margins(m, X, y) / lipschitz(m)\n",
    "\n",
    "\n",
    "def bartlett_complexity(m, X, y):\n",
    "    return 1 / normalized_margins(m, X, y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lipschitz(m_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lipschitz(m_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margins(m_smooth, X_train, y_train).mean(), lipschitz(m_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margins(m_noise, X_train, y_train).mean(), lipschitz(m_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "\n",
    "x = np.linspace(0, 0.5, 100)\n",
    "m_smooth_margins_density = gaussian_kde(normalized_margins(m_smooth, X_train, y_train))\n",
    "m_smooth_complexity = bartlett_complexity(m_smooth, X_train, y_train)\n",
    "plt.fill_between(x, m_smooth_margins_density(x), 0, alpha=0.5,\n",
    "                 label=f'smooth model (complexity: {m_smooth_complexity:0.1f})')\n",
    "m_noise_margins_density = gaussian_kde(normalized_margins(m_noise, X_train, y_train))\n",
    "m_noise_complexity = bartlett_complexity(m_noise, X_train, y_train)\n",
    "plt.fill_between(x, m_noise_margins_density(x), 0, alpha=0.5,\n",
    "                 label=f'hard/noisy model (complexity: {m_noise_complexity:0.1f})')\n",
    "plt.xlabel('Density of normalized margins (training set)')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity measures vs excess risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "from gzip import GzipFile\n",
    "\n",
    "\n",
    "with GzipFile('models.log.gz', 'r') as f:\n",
    "    records = [json.loads(line.strip()) for line in f]\n",
    "    models_df = pd.DataFrame.from_dict(records)\n",
    "\n",
    "with GzipFile('evaluations.log.gz', 'r') as f:\n",
    "    records = [json.loads(line.strip()) for line in f]\n",
    "    df = pd.DataFrame.from_dict(records)\n",
    "    \n",
    "df = df.merge(models_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(16, 6))\n",
    "# q = df.query(\"solver == 'lbfgs' and label_noise_rate == 0.1 and width == 256\").sort_values('depth')\n",
    "# q.plot(x='depth', y='train_acc', kind='line', ax=ax)\n",
    "# q.plot(x='depth', y='test_acc', kind='line', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(16, 6))\n",
    "# q = df.query(\"solver == 'lbfgs' and label_noise_rate == 0.1 and depth == 1\").sort_values('width')\n",
    "# q.plot(x='width', y='train_acc', kind='line', ax=ax)\n",
    "# q.plot(x='width', y='test_acc', kind='line', ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.query(\"excess_risk > 0.22 and bartlett_complexity_mean < 7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "q = df.query(\"train_acc >= 0.9 and width == 256\")\n",
    "depths = sorted(q['depth'].unique())\n",
    "colors = plt.cm.tab10(np.linspace(0, 0.5, len(depths)))\n",
    "for depth, color in zip(depths, colors):\n",
    "    q.query(f\"depth == '{depth}'\").plot(\n",
    "            x='lipschitz', y='excess_risk', kind='scatter',\n",
    "            c=color, s=50, label=f\"{depth} hidden layer\", ax=ax)\n",
    "ax.set_xlim((0, 300))\n",
    "ax.plot([0, 150], [0.07, 0.75])\n",
    "fig.suptitle(\"256 units per hidden-layer MLPs\")\n",
    "fig.savefig(\"lipschitz_depth.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "q = df.query(\"train_acc >= 0.9 and depth == 1\")\n",
    "widths = sorted(q['width'].unique())\n",
    "colors = plt.cm.tab10(np.linspace(0, 0.5, len(widths)))\n",
    "for width, color in zip(widths, colors):\n",
    "    q.query(f\"width == '{width}'\").plot(\n",
    "            x='lipschitz', y='excess_risk', kind='scatter',\n",
    "            c=color, s=50, label=f\"width: {width} units\", ax=ax)\n",
    "ax.set_xlim((0, 300))\n",
    "ax.plot([0, 150], [0.07, 0.75])\n",
    "fig.suptitle(\"1-hidden layer MLPs\");\n",
    "fig.savefig(\"lipschitz_width.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "solvers = q['solver'].unique()\n",
    "colors = plt.cm.tab10(np.linspace(0, 0.2, len(solvers)))\n",
    "\n",
    "q = df.query(\"train_acc >= .9\")\n",
    "for solver, color in zip(solvers, colors):\n",
    "    q.query(f\"solver == '{solver}'\").plot(\n",
    "            x='lipschitz', y='excess_risk', kind='scatter',\n",
    "            c=color, s=50, label=solver, ax=ax)\n",
    "ax.plot([0, 150], [0.07, 0.75])\n",
    "ax.set_xlim((0, 300));\n",
    "fig.savefig(\"lipschitz_optimizer.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "q = df.query(\"train_acc >= 0.9\")\n",
    "for solver, color in zip(solvers, colors):\n",
    "    q.query(f\"solver == '{solver}'\").plot(\n",
    "            x='bartlett_complexity_mean', y='excess_risk', kind='scatter',\n",
    "            c=color, s=50, label=solver, ax=ax)\n",
    "ax.set_xlabel('Lipschitz / margin')\n",
    "# ax.plot([0, 45], [0.23, 0.75])\n",
    "ax.set_xlim((0, 100));\n",
    "fig.savefig(\"lipschitz_margin_optimizer.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "q = df.query(\"train_acc >= 1\")\n",
    "q.plot(x='lipschitz', y='excess_risk', kind='scatter',\n",
    "       c=plt.cm.Blues(q['label_noise_rate']), s=50, ax=ax)\n",
    "ax.plot([0, 150], [0.07, 0.75], c='k')\n",
    "ax.set_xlim((0, 300))\n",
    "fig.suptitle('Impact of label noise (0% to 100%)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model_id in df['model_id'].unique():\n",
    "#     df.query('n_samples_train == 30')[df['model_id'] == model_id].plot('label_noise_rate', 'lipschitz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit regularization by SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
